---
title: A very simple explanation for the bias variance trade Off
author: Rachit Kinger
date: '2018-11-06'
slug: bias-variance-trade-off
categories:
  - data science
tags:
  - bias-variance trade off
  - bias in machine learning
  - variance in machine learning
  - statistical learning
subtitle: ''
---



<p>Reading through any statistical learning text one is bound to come across the bias-variance trade-off quite regularly. The concept is fundamental to understanding why certain models are better than others for a given problem. Here is a simple explanation of what we talk about when we talk about bias-variance trade off.</p>
<div id="what-is-variance-in-a-statistical-model-when-we-talk-of-bias-variance-trade-off" class="section level2">
<h2>What is variance in a statistical model when we talk of bias variance trade off?</h2>
<p>Variance = variance in the model if we had used a different training set. If we build a model that is highly tuned to be accurate on the given training set, then its parameters (or coefficients) are unique to the training set and hence will have little generalisability.</p>
</div>
<div id="what-is-bias-in-a-statistical-model-when-we-talk-of-bias-variance-trade-off" class="section level2">
<h2>What is bias in a statistical model when we talk of bias variance trade off?</h2>
<p>Bias = variance due the assumption of the model itself. Essentially, a model tries to approximate the ‘real’ relationship between independent and dependent variables by using mathematical relationships. These approximations may not always be true. For example, a linear regression model will only give us a linear relationship between independent and dependent variables even if the true relationship is non-linear.</p>
<p>There is a lot more one can say about bias-variance trade off and at the time of selecting your final model always spend a moment to think about how much of your variance is due to the ‘bias’ in the model and how much is due to ‘hyper-tuning’.</p>
</div>
